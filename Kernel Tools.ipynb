{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rustem\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\rustem\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "c:\\users\\rustem\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# Our Import Statements\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "from seaborn import heatmap\n",
    "from skimage.transform import rescale\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from tensorflow import random\n",
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "set_seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv-14x14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 12, 12, 2)         20        \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 6, 6, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 2, 2, 120)         6120      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 84)                40404     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                2380      \n",
      "=================================================================\n",
      "Total params: 48,924\n",
      "Trainable params: 48,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "model_LeNet5 = Sequential(name = 'Conv-14x14')\n",
    "\n",
    "model_LeNet5.add(layers.Conv2D(trainable=True, filters=2, kernel_size=(3,3), padding='valid', activation='relu', input_shape=(14, 14, 1)))\n",
    "model_LeNet5.add(layers.AveragePooling2D(pool_size=(2,2)))\n",
    "\n",
    "model_LeNet5.add(layers.Conv2D(filters=120, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "model_LeNet5.add(layers.Flatten())\n",
    "\n",
    "model_LeNet5.add(layers.Dense(84, activation='relu'))\n",
    "model_LeNet5.add(layers.Dense(28, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "model_LeNet5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 2) dtype=float32, numpy=\n",
       " array([[[[1., 9.]],\n",
       " \n",
       "         [[2., 8.]],\n",
       " \n",
       "         [[3., 7.]]],\n",
       " \n",
       " \n",
       "        [[[4., 6.]],\n",
       " \n",
       "         [[5., 5.]],\n",
       " \n",
       "         [[6., 4.]]],\n",
       " \n",
       " \n",
       "        [[[7., 3.]],\n",
       " \n",
       "         [[8., 2.]],\n",
       " \n",
       "         [[9., 1.]]]], dtype=float32)>,\n",
       " <tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def changeKernels(layer, filters):\n",
    "    c = layer.get_weights().copy()\n",
    "    for i in range(len(filters)):\n",
    "        c[0][:,:,0,:][:,:,i] = filters[i]\n",
    "    layer.set_weights(c)\n",
    "    \n",
    "filter1 = np.array([\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9]\n",
    "    ], dtype='float32')\n",
    "filter2 = np.array([\n",
    "        [9, 8, 7],\n",
    "        [6, 5, 4],\n",
    "        [3, 2, 1]\n",
    "    ], dtype='float32')\n",
    "changeKernels(model_LeNet5.layers[0], [filter1, filter2])\n",
    "model_LeNet5.layers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot2D(row, col, images, title='', x_ticks=[], y_ticks=[]):\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(row, col, i+1)\n",
    "        plt.imshow(images[i], 'gray')\n",
    "        plt.title(title)\n",
    "        plt.xticks([])\n",
    "        plt.xticks([])\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def plot_kernels_weights(weights, row, col):\n",
    "    #weights, bias= layer.get_weights()\n",
    "    f_min, f_max = weights.min(), weights.max()\n",
    "    filters = (weights - f_min) / (f_max - f_min)  \n",
    "    filters_new = []\n",
    "    for i in range(filters.shape[3]):\n",
    "        filters_new.append(filters[:,:,:, i][:,:, 0])\n",
    "        #print(filters[:,:,:, i][:,:, 0])\n",
    "    plot2D(row, col, filters_new)\n",
    "    return\n",
    "\n",
    "def plot_kernels_layer(layer, row, col):\n",
    "    plot_kernels_weights(layer.get_weights()[0], row, col)\n",
    "\n",
    "def plot_kernels_compare(both_weights):   \n",
    "    filters_new = []\n",
    "    for weights in both_weights:\n",
    "        f_min, f_max = weights.min(), weights.max()\n",
    "        filters = (weights - f_min) / (f_max - f_min)  \n",
    "        for i in range(filters.shape[3]):\n",
    "            filters_new.append(filters[:,:,:, i][:,:, 0])\n",
    "    plot2D(len(both_weights), both_weights[0].shape[3], filters_new)\n",
    "    return\n",
    "\n",
    "plot_kernels_layer(model_LeNet5.layers[0], 2, 1)\n",
    "\n",
    "both_weights = []\n",
    "\n",
    "#BEFORE training\n",
    "both_weights.append(model.layers[0].get_weights)\n",
    "\n",
    "# AFTER training\n",
    "both_weights.append(model.layers[0].get_weights)\n",
    "\n",
    "plot_kernels_compare(both_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[1., 9.]],\n",
       " \n",
       "         [[2., 8.]],\n",
       " \n",
       "         [[3., 7.]]],\n",
       " \n",
       " \n",
       "        [[[4., 6.]],\n",
       " \n",
       "         [[5., 5.]],\n",
       " \n",
       "         [[6., 4.]]],\n",
       " \n",
       " \n",
       "        [[[7., 3.]],\n",
       " \n",
       "         [[8., 2.]],\n",
       " \n",
       "         [[9., 1.]]]], dtype=float32), array([0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.get_weights().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Real work with Kernels ***\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# Our Import Statements\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "from seaborn import heatmap\n",
    "from skimage.transform import rescale\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from tensorflow import random\n",
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "set_seed(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for READING IMAGES/LABELS\n",
    "\n",
    "def padding(image, shape):\n",
    "    \n",
    "    def seperate_pad_width(width):\n",
    "        return (width // 2, ceil(width / 2))\n",
    "    \n",
    "    padding_axis = [seperate_pad_width(abs(image.shape[0] - shape[0])),\n",
    "                    seperate_pad_width(abs(image.shape[1] - shape[1]))]\n",
    "    image = np.pad(image, (padding_axis[0], padding_axis[1], (0,0)), constant_values = 0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def resample(image, shape_read_file, shape_with_padding):\n",
    "   \n",
    "    ratio = min(shape_read_file[0] / image.shape[0], shape_read_file[1] / image.shape[1])\n",
    "    image = rescale(image, ratio, order = 1, preserve_range = True, multichannel = True)\n",
    "    \n",
    "    image = padding(image, shape_with_padding) # * shape    \n",
    "    \n",
    "    return image\n",
    "\n",
    "def read_file(path, shape_read_file, shape_with_padding, is_gray=False, is_gonna_be_resampled=True):\n",
    "    X, Y = [], []\n",
    "    file = open(path, 'r')\n",
    "    file.readline() # ignore  header\n",
    "    \n",
    "    for line in file:\n",
    "        temp = line[ :-1].split('#')\n",
    "        \n",
    "        image_height = int(temp[3])\n",
    "        image_width = int(temp[4])\n",
    "        \n",
    "        if is_gray:\n",
    "            image = temp[-1].split(';')\n",
    "        else:\n",
    "            image = [(ord(i) & 1) * 255 for i in temp[-1]]\n",
    "        image = np.array(image, dtype = 'uint8')\n",
    "        \n",
    "        image.resize(image_height, image_width, 1)\n",
    "        if is_gonna_be_resampled:\n",
    "            image = resample(image, shape_read_file, shape_with_padding)\n",
    "            \n",
    "        label = int(temp[0])\n",
    "        \n",
    "        X.append(image)\n",
    "        Y.append(label)\n",
    "        \n",
    "    image_count = len(Y)\n",
    "    if not is_gonna_be_resampled:\n",
    "        width = max(len(im[0]) for im in X)\n",
    "        height = max(len(im) for im in X)\n",
    "        #print(width, height)\n",
    "        for i in range(len(X)):\n",
    "            X[i] = resample(X[i], (width, height, 1), (width, height, 1))\n",
    "    \n",
    "    X = np.array(X, dtype = 'uint8')\n",
    "    Y = to_categorical(Y, dtype = 'uint8') # num_classes = 10,  * , num_classes = len(Y)\n",
    "    return X,Y\n",
    "\n",
    "def read_preapare_db(path, shape_read_file, shape_with_padding):\n",
    "    is_gray = False #'gray' in str(path)\n",
    "    X, Y = read_file(path, shape_read_file, shape_with_padding, is_gray)\n",
    "    #X = vpadding(X, shape)\n",
    "    X = X.astype('float32')\n",
    "    X /= 255\n",
    "    return X,Y\n",
    "\n",
    "def read_preapare_db_original_size(path, shape_read_file=(20,20,1), shape_with_padding=(32,32,1)):\n",
    "    is_gray = False #'gray' in str(path)\n",
    "    X, Y = read_file(path, shape_read_file, shape_with_padding, is_gray, is_gonna_be_resampled=False)\n",
    "    #X = vpadding(X, shape)\n",
    "    X = X.astype('float32')\n",
    "    X /= 255\n",
    "    return X,Y\n",
    "\n",
    "def read_prepare_db_pdc(path, shape_read_file, shape_with_padding, is_gray=False):\n",
    "    X, Y = [], []\n",
    "    file = open(path, 'r')\n",
    "    file.readline() # ignore  header\n",
    "    \n",
    "    for line in file:\n",
    "        temp = line[ :-1].split('#')\n",
    "        \n",
    "        image_height = int(temp[3])\n",
    "        image_width = int(temp[4])\n",
    "        \n",
    "        image = temp[-1].split(';')\n",
    "        \"\"\"\n",
    "        if is_gray:\n",
    "            image = temp[-1].split(';')\n",
    "        else:\n",
    "            image = [(ord(i) & 1) * 255 for i in temp[-1]]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            image = np.array(image, dtype = 'float32')\n",
    "        except ValueError as e:\n",
    "            print(temp[-1])\n",
    "        \n",
    "        #image = np.array(image, dtype = 'float32')\n",
    "        \n",
    "        #image.resize(image_height, image_width, 1)\n",
    "        #image = resample(image, shape_read_file, shape_with_padding)\n",
    "        \n",
    "        label = int(temp[0])\n",
    "        \n",
    "        X.append(image)\n",
    "        Y.append(label)\n",
    "        \n",
    "    #image_count = len(Y)\n",
    "    X = np.array(X, dtype = 'float32')\n",
    "    Y = to_categorical(Y, dtype = 'uint8') # num_classes = 10, \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare TRAIN/TEST DATA\n",
    "\n",
    "def augment(x_train_original, y_train_original, n, is_shuffled = True):\n",
    "    datagen = ImageDataGenerator(featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=10,\n",
    "      width_shift_range=0,\n",
    "      height_shift_range=0,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "    it = datagen.flow(x_train_original, y_train_original, batch_size=14000, shuffle=is_shuffled, seed=0) #x_train_32x32 #.reshape(14000, 32, 32, 1)\n",
    "    x, y = it.next()\n",
    "    for i in range(n-1):\n",
    "        temp_x, temp_y = it.next()\n",
    "        x = np.concatenate((x, temp_x), axis=0)\n",
    "        y = np.concatenate((y, temp_y), axis=0)\n",
    "        print(temp_y.shape)\n",
    "        print(x.shape)\n",
    "    return x, y #data\n",
    "\n",
    "def extract_spec_size(x_train_original, shape_read_file, shape_with_padding):\n",
    "    x_train_new = list()\n",
    "    for i in range(len(x_train_original)):\n",
    "        x_train_new.append(resample(x_train_original[i], shape_read_file, shape_with_padding))\n",
    "    x_train_new = np.array(x_train_new, dtype = 'float32')\n",
    "    return x_train_new\n",
    "\n",
    "aug_size = 5\n",
    "# PREPARE 32x32, 20x20 from AUGMENTED\n",
    "x_train_original, y_train_original = read_preapare_db_original_size(Path('datasets/500-100/HandChars_500_woI_Train.txt').absolute())\n",
    "x_train_original, y_train_original = augment(x_train_original, y_train_original, n=aug_size, is_shuffled = False) # n * 14000\n",
    "\n",
    "y_train = y_train_original\n",
    "x_train_32x32 = extract_spec_size(x_train_original, (20, 20, 1), (32, 32, 1))\n",
    "\n",
    "# 100 Test\n",
    "x_test_32x32, y_test = read_preapare_db(Path('datasets/500-100/HandChars_100_Test.txt').absolute(), (20, 20, 1), (32, 32, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Model:\n",
    "    models = []\n",
    "\n",
    "    def __init__(self, model,\n",
    "                 seed, pooling,\n",
    "                 name, params_count, epochs,\n",
    "                 acc_train, loss_train,\n",
    "                 acc_test, loss_test):\n",
    "        self.model = model\n",
    "        self.seed = seed\n",
    "        self.pooling = pooling\n",
    "        self.name = name\n",
    "        self.params_count = params_count\n",
    "        self.epochs = epochs\n",
    "        self.acc_train = acc_train\n",
    "        self.loss_train = loss_train\n",
    "        self.acc_test = acc_test\n",
    "        self.loss_test = loss_test\n",
    "        \n",
    "        models.append(self)\n",
    "        \n",
    "    def get_by_acc_test():\n",
    "        return models[np.argmax([my_models.acc_test for my_models in models])]\n",
    "\n",
    "    def get_by_loss_test():\n",
    "        return models[np.argmin([my_models.loss_test for my_models in models])]\n",
    "    \n",
    "    def get_by_loss_train():\n",
    "        return models[np.argmin([my_models.loss_train for my_models in models])]\n",
    "        #return f\"{self.name} is {self.age} years old\"\n",
    "        \n",
    "    def add_model(model):\n",
    "        models.append(model)\n",
    "    \n",
    "    def all_models():\n",
    "        return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_2x2(Y_classes, Y_pred_classes, label_map, model_name, seed):\n",
    "    \n",
    "    confusion_mtx = confusion_matrix(Y_classes, Y_pred_classes) \n",
    "    dpi_num = 400\n",
    "    plt.subplots(figsize = (28, 28)) #3.6, 3   1024/dpi_num, 1024/dpi_num\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(model_name + ', Seed: ' + str(seed))\n",
    "\n",
    "    heatmap(confusion_mtx, annot = True, linewidths = 0.02, cmap = \"Greys\", linecolor = \"gray\",\n",
    "            xticklabels = label_map, yticklabels = label_map, fmt='d')\n",
    "    plt.savefig('output/expertising_model/conf_azhand_custom_conv_seed-{1}_{0}.png'.format(model_name, seed), dpi=dpi_num)\n",
    "    \n",
    "    #plt.show()\n",
    "    \n",
    "    return\n",
    "    \n",
    "def changeKernels(layer, filters):\n",
    "    c = layer.get_weights().copy()\n",
    "    for i in range(len(filters)):\n",
    "        c[0][:,:,0,:][:,:,i] = filters[i]\n",
    "    layer.set_weights(c)\n",
    "\n",
    "# *** EDIT FILTERS HERE ***\n",
    "filter1 = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "], dtype='float32')\n",
    "\n",
    "filter2 = np.array([\n",
    "    [9, 8, 7],\n",
    "    [6, 5, 4],\n",
    "    [3, 2, 1]\n",
    "], dtype='float32')\n",
    "\n",
    "filters = [filter1, filter2]\n",
    "\n",
    "EPOCHS = 200\n",
    "callback = EarlyStopping(monitor='loss', min_delta=0.01, patience=5)\n",
    "results_df = pd.DataFrame(columns=['Seed', 'Pooling2D', 'Model', 'Param-Count', 'Epochs',\n",
    "                                   'Accuracy-Train', 'Loss-Train', 'Accuracy-Test', 'Loss-Test'])\n",
    "models = []\n",
    "for Pooling2D in (layers.AveragePooling2D,layers.MaxPooling2D):\n",
    "    for seed in range(5):   #layers.MaxPooling2D, \n",
    "        set_seed(seed)\n",
    "        \n",
    "        # conv 1\n",
    "        keras.backend.clear_session()\n",
    "        model_LeNet5 = Sequential(name = str('LeNet5' + '-' + str(Pooling2D)[47:-2] + '_seed-' + str(seed)))\n",
    "        \n",
    "        model_LeNet5.add(layers.Conv2D(filters=6,  kernel_size=(5,5), padding='valid', activation='relu', input_shape=(32, 32, 1)))\n",
    "        \n",
    "        changeKernels(model_LeNet5.layers[0], filters)\n",
    "        \n",
    "        model_LeNet5.add(Pooling2D(pool_size = (2,2)))\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "        model_LeNet5.add(Pooling2D(pool_size=(2,2)))\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=120, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "        model_LeNet5.add(layers.Flatten())\n",
    "\n",
    "        model_LeNet5.add(layers.Dense(84, activation='relu'))\n",
    "        model_LeNet5.add(layers.Dense(28, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "        model_LeNet5.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "        fitted_results = model_LeNet5.fit(x_train_32x32, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "        \n",
    "        loss_train_LeNet5, acc_train_LeNet5 = model_LeNet5.evaluate(x_train_32x32, y_train, verbose=0)\n",
    "        loss_test_LeNet5, acc_test_LeNet5 = model_LeNet5.evaluate(x_test_32x32, y_test, verbose=0)\n",
    "        \n",
    "        results_df.loc[len(results_df)] = [str(seed), str(Pooling2D)[47:-2], str(model_LeNet5.name),\n",
    "                                           str(model_LeNet5.count_params()),\n",
    "                                           len(fitted_results.history['loss']),\n",
    "                                           acc_train_LeNet5, loss_train_LeNet5,\n",
    "                                           acc_test_LeNet5, loss_test_LeNet5]\n",
    "        print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "        #models.append()\n",
    "        My_Model.add_model(My_Model(model_LeNet5,\n",
    "            seed, str(Pooling2D)[47:-2], str(model_LeNet5.name), model_LeNet5.count_params(),\n",
    "            len(fitted_results.history['loss']),\n",
    "            acc_train_LeNet5, loss_train_LeNet5,\n",
    "            acc_test_LeNet5, loss_test_LeNet5\n",
    "        ))\n",
    "        \n",
    "        y_pred_classes = np.argmax(model_LeNet5.predict(x_test_32x32), axis = 1) \n",
    "        y_classes = np.argmax(y_test, axis = 1)\n",
    "        conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_LeNet5.name + '-' + str(Pooling2D)[47:-2], seed)\n",
    "    \n",
    "    results_df.to_csv(r'output/expertising_model/Results_train-custom-CONV-added.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
