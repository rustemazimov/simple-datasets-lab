{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-23T06:15:15.337993Z",
     "iopub.status.busy": "2021-06-23T06:15:15.337522Z",
     "iopub.status.idle": "2021-06-23T06:15:44.169530Z",
     "shell.execute_reply": "2021-06-23T06:15:44.168592Z",
     "shell.execute_reply.started": "2021-06-23T06:15:15.337897Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rustem\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\rustem\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\n",
      "c:\\users\\rustem\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "# Our Import Statements\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "from seaborn import heatmap\n",
    "from skimage.transform import rescale\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from tensorflow import random\n",
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "set_seed(4)\n",
    "\n",
    "def padding(image, shape):\n",
    "    \n",
    "    def seperate_pad_width(width):\n",
    "        return (width // 2, ceil(width / 2))\n",
    "    \n",
    "    padding_axis = [seperate_pad_width(abs(image.shape[0] - shape[0])),\n",
    "                    seperate_pad_width(abs(image.shape[1] - shape[1]))]\n",
    "    image = np.pad(image, (padding_axis[0], padding_axis[1], (0,0)), constant_values = 0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def resample(image, shape_read_file, shape_with_padding):\n",
    "   \n",
    "    ratio = min(shape_read_file[0] / image.shape[0], shape_read_file[1] / image.shape[1])\n",
    "    image = rescale(image, ratio, order = 1, preserve_range = True, multichannel = True)\n",
    "    \n",
    "    image = padding(image, shape_with_padding) # * shape    \n",
    "    \n",
    "    return image\n",
    "\n",
    "def read_file(path, shape_read_file, shape_with_padding, is_gray=False, is_gonna_be_resampled=True):\n",
    "    X, Y = [], []\n",
    "    file = open(path, 'r')\n",
    "    file.readline() # ignore  header\n",
    "    \n",
    "    for line in file:\n",
    "        temp = line[ :-1].split('#')\n",
    "        \n",
    "        image_height = int(temp[3])\n",
    "        image_width = int(temp[4])\n",
    "        \n",
    "        if is_gray:\n",
    "            image = temp[-1].split(';')\n",
    "        else:\n",
    "            image = [(ord(i) & 1) * 255 for i in temp[-1]]\n",
    "        image = np.array(image, dtype = 'uint8')\n",
    "        \n",
    "        image.resize(image_height, image_width, 1)\n",
    "        if is_gonna_be_resampled:\n",
    "            image = resample(image, shape_read_file, shape_with_padding)\n",
    "            \n",
    "        label = int(temp[0])\n",
    "        \n",
    "        X.append(image)\n",
    "        Y.append(label)\n",
    "        \n",
    "    image_count = len(Y)\n",
    "    if not is_gonna_be_resampled:\n",
    "        width = max(len(im[0]) for im in X)\n",
    "        height = max(len(im) for im in X)\n",
    "        #print(width, height)\n",
    "        for i in range(len(X)):\n",
    "            X[i] = resample(X[i], (width, height, 1), (width, height, 1))\n",
    "    \n",
    "    X = np.array(X, dtype = 'uint8')\n",
    "    Y = to_categorical(Y, dtype = 'uint8') # num_classes = 10,  *\n",
    "    return X,Y\n",
    "\n",
    "def read_preapare_db(path, shape_read_file, shape_with_padding):\n",
    "    is_gray = False #'gray' in str(path)\n",
    "    X, Y = read_file(path, shape_read_file, shape_with_padding, is_gray)\n",
    "    #X = vpadding(X, shape)\n",
    "    X = X.astype('float32')\n",
    "    X /= 255\n",
    "    return X,Y\n",
    "\n",
    "def read_preapare_db_original_size(path, shape_read_file=(20,20,1), shape_with_padding=(32,32,1)):\n",
    "    is_gray = False #'gray' in str(path)\n",
    "    X, Y = read_file(path, shape_read_file, shape_with_padding, is_gray, is_gonna_be_resampled=False)\n",
    "    #X = vpadding(X, shape)\n",
    "    X = X.astype('float32')\n",
    "    X /= 255\n",
    "    return X,Y\n",
    "\n",
    "def read_prepare_db_pdc(path, shape_read_file, shape_with_padding, is_gray=False):\n",
    "    X, Y = [], []\n",
    "    file = open(path, 'r')\n",
    "    file.readline() # ignore  header\n",
    "    \n",
    "    for line in file:\n",
    "        temp = line[ :-1].split('#')\n",
    "        \n",
    "        image_height = int(temp[3])\n",
    "        image_width = int(temp[4])\n",
    "        \n",
    "        image = temp[-1].split(';')\n",
    "        \"\"\"\n",
    "        if is_gray:\n",
    "            image = temp[-1].split(';')\n",
    "        else:\n",
    "            image = [(ord(i) & 1) * 255 for i in temp[-1]]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            image = np.array(image, dtype = 'float32')\n",
    "        except ValueError as e:\n",
    "            print(temp[-1])\n",
    "        \n",
    "        #image = np.array(image, dtype = 'float32')\n",
    "        \n",
    "        #image.resize(image_height, image_width, 1)\n",
    "        #image = resample(image, shape_read_file, shape_with_padding)\n",
    "        \n",
    "        label = int(temp[0])\n",
    "        \n",
    "        X.append(image)\n",
    "        Y.append(label)\n",
    "        \n",
    "    #image_count = len(Y)\n",
    "    X = np.array(X, dtype = 'float32')\n",
    "    Y = to_categorical(Y, dtype = 'uint8') # num_classes = 10, \n",
    "    return X,Y\n",
    "\n",
    "# 500 Train\n",
    "#x_train_32x32, y_train = read_preapare_db(Path('datasets/500-100/HandChars_500_woI_Train.txt').absolute(), (20, 20, 1), (32, 32, 1))\n",
    "#x_train_14x14, _ = read_preapare_db(Path('datasets/500-100/HandChars_500_woI_Train.txt').absolute(), (10, 10, 1), (14, 14, 1))\n",
    "#x_train_20x20, y_train_20x20_550 = read_preapare_db(Path('datasets/500-100/HandChars_500_woI_Train.txt').absolute(), (20, 20, 1), (20, 20, 1))\n",
    "\n",
    "# 250 Train\n",
    "#x_train_32x32, y_train = read_preapare_db(Path('datasets/250/HandChars_250_woI_Train.txt').absolute(), (20, 20, 1), (32, 32, 1))\n",
    "#x_train_14x14, _ = read_preapare_db(Path('datasets/250/HandChars_250_woI_Train.txt').absolute(), (10, 10, 1), (14, 14, 1))\n",
    "#x_train_20x20, y_train_20x20_550 = read_preapare_db(Path('datasets/250/HandChars_250_woI_Train.txt').absolute(), (20, 20, 1), (20, 20, 1))\n",
    "\n",
    "\n",
    "# 500 Train PDC\n",
    "x_train_pdc, y_train_pdc = read_prepare_db_pdc(Path('datasetsforfinalresults/HandChars32_500_woI_Train_PDC4428.txt').absolute(), (0, 256, 1), (0, 256, 1))\n",
    "\n",
    "# 250 Train PDC\n",
    "#x_train_pdc, y_train_pdc = read_prepare_db_pdc(Path('datasets/250/HandChars32_250_woI_Train_PDC4428.txt').absolute(), (0, 256, 1), (0, 256, 1))\n",
    "\n",
    "\n",
    "# 100 Test\n",
    "x_test_32x32, y_test = read_preapare_db(Path('datasetsforfinalresults/HandChars_100_Test.txt').absolute(), (20, 20, 1), (32, 32, 1))\n",
    "x_test_20x20, y_test_20x20 = read_preapare_db(Path('datasetsforfinalresults/HandChars_100_Test.txt').absolute(), (20, 20, 1), (20, 20, 1))\n",
    "x_test_14x14, _ = read_preapare_db(Path('datasetsforfinalresults/HandChars_100_Test.txt').absolute(), (10, 10, 1), (14, 14, 1))\n",
    "\n",
    "# 100 Test PDC\n",
    "x_test_pdc, y_test_pdc = read_prepare_db_pdc(Path('datasetsforfinalresults/HandChars32_100_Test_PDC4428.txt').absolute(), (0, 256, 1), (0, 256, 1))\n",
    "\n",
    "#x_train_20x20 = x_train_20x20.reshape(x_train_20x20.shape[0], 400)\n",
    "x_test_20x20 = x_test_20x20.reshape(x_test_20x20.shape[0], 400)\n",
    "\n",
    "# For all ks\n",
    "\n",
    "# AUGMENT and PREPARE all datasets\n",
    "\n",
    "def augment(x_train_original, y_train_original, n, is_shuffled = True):\n",
    "    datagen = ImageDataGenerator(featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=10,\n",
    "      width_shift_range=0,\n",
    "      height_shift_range=0,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "    \"\"\"\n",
    "    horizontal_flip=True,\n",
    "\n",
    "    \"\"\"\n",
    "    it = datagen.flow(x_train_original, y_train_original, batch_size=14000, shuffle=is_shuffled, seed=0) #x_train_32x32 #.reshape(14000, 32, 32, 1)\n",
    "    x, y = x_train_original, y_train_original #it.next()\n",
    "    for i in range(n-1):\n",
    "        temp_x, temp_y = it.next()\n",
    "        x = np.concatenate((x, temp_x), axis=0)\n",
    "        y = np.concatenate((y, temp_y), axis=0) #\n",
    "    #data = np.concatenate((it.next(), it.next()), axis=0)\n",
    "    #for i in range(n-2):\n",
    "     #   data = np.concatenate((data, it.next()), axis=0)\n",
    "    #plt.imshow(resample(data[13999*3], (20, 20, 1), (32, 32, 1)).reshape(32, 32),cmap='Greys')\n",
    "    return x, y #data\n",
    "\n",
    "def extract_spec_size(x_train_original, shape_read_file, shape_with_padding):\n",
    "    x_train_new = list()\n",
    "    for i in range(len(x_train_original)):\n",
    "        x_train_new.append(resample(x_train_original[i], shape_read_file, shape_with_padding))\n",
    "    x_train_new = np.array(x_train_new, dtype = 'float32')\n",
    "    return x_train_new\n",
    "\n",
    "# =============== AUGMENT SIZE ===============\n",
    "#aug_size = 2 #2, 3, 4\n",
    "# =============== AUGMENT SIZE ===============\n",
    "\n",
    "# PREPARE 32x32, 20x20 from AUGMENTED\n",
    "\n",
    "# AUGMENTED Train PDC\n",
    "\n",
    "# PREPARED PDC files for {seed = 0}\n",
    "aug_switch = {\n",
    "    2:'datasetsforfinalresults/GENERATED_HandChars_28k_32x32_Train_PDC4428_saved.txt',\n",
    "    3:'datasetsforfinalresults/GENERATED_HandChars_42k_32x32_Train_PDC4428_saved.txt',\n",
    "    5:'datasetsforfinalresults/GENERATED_HandChars_70k_32x32_Train_PDC4428_saved.txt'\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T06:16:00.996404Z",
     "iopub.status.busy": "2021-06-23T06:16:00.996065Z"
    }
   },
   "outputs": [],
   "source": [
    "def conf_2x2(Y_classes, Y_pred_classes, label_map, model_name, seed):\n",
    "    return\n",
    "    \n",
    "# Tezis 1.2\n",
    "\n",
    "EPOCHS = 200\n",
    "callback = EarlyStopping(monitor='loss', min_delta=0.01, patience=5)\n",
    "\n",
    "#models = []\n",
    "for aug_size in (5, 2, 3): #2, \n",
    "    results_df = pd.DataFrame(columns=['Seed', 'Pooling2D', 'Model', 'Param-Count',\n",
    "                                   'Accuracy-Train', 'Loss-Train', 'Accuracy-Test', 'Loss-Test'])\n",
    "    print('AUGMENTATION STARTING...')\n",
    "    x_train_original, y_train_original = read_preapare_db_original_size(Path('datasetsforfinalresults/HandChars_500_woI_Train.txt').absolute())\n",
    "    print('DATA READ!')\n",
    "    x_train_original, y_train_original = augment(x_train_original, y_train_original, n=aug_size, is_shuffled = False) # n * 14000\n",
    "    print('AUGMENTATION DONE', str(x_train_original.shape[0]))\n",
    "    x_train_pdc, y_train_pdc = read_prepare_db_pdc(Path(aug_switch.get(aug_size, '')).absolute(), (0, 256, 1), (0, 256, 1))\n",
    "    print('PDC READ')\n",
    "    y_train = y_train_original\n",
    "    x_train_32x32 = extract_spec_size(x_train_original, (20, 20, 1), (32, 32, 1))\n",
    "    x_train_14x14 = extract_spec_size(x_train_original, (10, 10, 1), (14, 14, 1))\n",
    "    x_train_20x20 = extract_spec_size(x_train_original, (20, 20, 1), (20, 20, 1))\n",
    "    \n",
    "    #x_train_new = extract_spec_size(x_train_original, (32, 32, 1), (32, 32, 1)) #(20, 20, 1), (32, 32, 1)\n",
    "    #x_train_t = x_train_new\n",
    "    #y_train_t = y_train_new\n",
    "    #plt.imshow(x_train_new[14000 + 455*28].reshape(32, 32),cmap='Greys')\n",
    "    #write_to_file(Path('datasets/500-100/GENERATED_HandChars_' + str(n_i*14) + 'k_32x32_Train-18-iyun.txt').absolute(), x_train_new, y_train_new)\n",
    "    \n",
    "\n",
    "\n",
    "    x_train_20x20 = x_train_20x20.reshape(x_train_20x20.shape[0], 400)\n",
    "    print('SIZES DONE!')\n",
    "    for seed in range(5):\n",
    "        for Pooling2D in (layers.AveragePooling2D,layers.MaxPooling2D):   #layers.MaxPooling2D, \n",
    "            set_seed(seed)\n",
    "\n",
    "            # conv 1\n",
    "            keras.backend.clear_session()\n",
    "            model_LeNet5 = Sequential(name = 'LeNet5')\n",
    "\n",
    "            model_LeNet5.add(layers.Conv2D(filters=6,  kernel_size=(5,5), padding='valid', activation='relu', input_shape=(32, 32, 1)))\n",
    "            model_LeNet5.add(Pooling2D(pool_size = (2,2)))\n",
    "\n",
    "            model_LeNet5.add(layers.Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "            model_LeNet5.add(Pooling2D(pool_size=(2,2)))\n",
    "\n",
    "            model_LeNet5.add(layers.Conv2D(filters=120, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "            model_LeNet5.add(layers.Flatten())\n",
    "\n",
    "            model_LeNet5.add(layers.Dense(84, activation='relu'))\n",
    "            model_LeNet5.add(layers.Dense(28, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "            model_LeNet5.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "            model_LeNet5.fit(x_train_32x32, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "            #kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "            # len(fitted_results.history['loss'])\n",
    "            loss_train_LeNet5, acc_train_LeNet5 = model_LeNet5.evaluate(x_train_32x32, y_train, verbose=0)\n",
    "            loss_test_LeNet5, acc_test_LeNet5 = model_LeNet5.evaluate(x_test_32x32, y_test, verbose=0)\n",
    "\n",
    "            #models.append(model_LeNet5)\n",
    "            #print(acc_train_LeNet5, '^', acc_test_LeNet5)\n",
    "\n",
    "            results_df.loc[len(results_df)] = [str(seed), str(Pooling2D)[47:-2], str(model_LeNet5.name),\n",
    "                                               str(model_LeNet5.count_params()),\n",
    "                                               acc_train_LeNet5, loss_train_LeNet5,\n",
    "                                               acc_test_LeNet5, loss_test_LeNet5]\n",
    "            print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "            y_pred_classes = np.argmax(model_LeNet5.predict(x_test_32x32), axis = 1) \n",
    "            y_classes = np.argmax(y_test, axis = 1)\n",
    "            conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_LeNet5.name + '-' + str(Pooling2D)[47:-2]\n",
    "                     , seed)\n",
    "\n",
    "            # conv 2\n",
    "            keras.backend.clear_session()\n",
    "            model_LeNet5 = Sequential(name = 'Conv-14x14')\n",
    "\n",
    "            model_LeNet5.add(layers.Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='relu', input_shape=(14, 14, 1)))\n",
    "            model_LeNet5.add(Pooling2D(pool_size=(2,2)))\n",
    "\n",
    "            model_LeNet5.add(layers.Conv2D(filters=120, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "            model_LeNet5.add(layers.Flatten())\n",
    "\n",
    "            model_LeNet5.add(layers.Dense(84, activation='relu'))\n",
    "            model_LeNet5.add(layers.Dense(28, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "            model_LeNet5.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "            model_LeNet5.fit(x_train_14x14, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "\n",
    "            loss_train_LeNet5, acc_train_LeNet5 = model_LeNet5.evaluate(x_train_14x14, y_train, verbose=0)\n",
    "            loss_test_LeNet5, acc_test_LeNet5 = model_LeNet5.evaluate(x_test_14x14, y_test, verbose=0)\n",
    "\n",
    "            results_df.loc[len(results_df)] = [str(seed), str(Pooling2D)[47:-2], str(model_LeNet5.name),\n",
    "                                               str(model_LeNet5.count_params()),\n",
    "                                               acc_train_LeNet5, loss_train_LeNet5,\n",
    "                                               acc_test_LeNet5, loss_test_LeNet5]\n",
    "            print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "            y_pred_classes = np.argmax(model_LeNet5.predict(x_test_14x14), axis = 1) \n",
    "            y_classes = np.argmax(y_test, axis = 1)\n",
    "            conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_LeNet5.name + '-' + str(Pooling2D)[47:-2]\n",
    "                     , seed)\n",
    "\n",
    "            # conv 3\n",
    "            keras.backend.clear_session()\n",
    "            model_LeNet5 = Sequential(name = 'LeNet5-filters_6-8')\n",
    "\n",
    "            model_LeNet5.add(layers.Conv2D(filters=6,  kernel_size=(5,5), padding='valid', activation='relu', input_shape=(32, 32, 1)))\n",
    "            model_LeNet5.add(Pooling2D(pool_size = (2,2)))\n",
    "\n",
    "            model_LeNet5.add(layers.Conv2D(filters=8, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "            model_LeNet5.add(Pooling2D(pool_size=(2,2)))\n",
    "\n",
    "            model_LeNet5.add(layers.Conv2D(filters=120, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "            model_LeNet5.add(layers.Flatten())\n",
    "\n",
    "            model_LeNet5.add(layers.Dense(84, activation='relu'))\n",
    "            model_LeNet5.add(layers.Dense(28, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "            model_LeNet5.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "            model_LeNet5.fit(x_train_32x32, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "\n",
    "            loss_train_LeNet5, acc_train_LeNet5 = model_LeNet5.evaluate(x_train_32x32, y_train, verbose=0)\n",
    "            loss_test_LeNet5, acc_test_LeNet5 = model_LeNet5.evaluate(x_test_32x32, y_test, verbose=0)\n",
    "\n",
    "            #print(acc_train_LeNet5, '^', acc_test_LeNet5)\n",
    "\n",
    "            results_df.loc[len(results_df)] = [str(seed), str(Pooling2D)[47:-2], str(model_LeNet5.name),\n",
    "                                               str(model_LeNet5.count_params()),\n",
    "                                               acc_train_LeNet5, loss_train_LeNet5,\n",
    "                                               acc_test_LeNet5, loss_test_LeNet5]\n",
    "            print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "            y_pred_classes = np.argmax(model_LeNet5.predict(x_test_32x32), axis = 1) \n",
    "            y_classes = np.argmax(y_test, axis = 1)\n",
    "            conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_LeNet5.name + '-' + str(Pooling2D)[47:-2]\n",
    "                     , seed)\n",
    "\n",
    "            # conv 4\n",
    "            keras.backend.clear_session()\n",
    "            model_LeNet5 = Sequential(name = 'LeNet5-filters_6-4')\n",
    "\n",
    "            model_LeNet5.add(layers.Conv2D(filters=6,  kernel_size=(5,5), padding='valid', activation='relu', input_shape=(32, 32, 1)))\n",
    "            model_LeNet5.add(Pooling2D(pool_size = (2,2)))\n",
    "\n",
    "            model_LeNet5.add(layers.Conv2D(filters=4, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "            model_LeNet5.add(Pooling2D(pool_size=(2,2)))\n",
    "\n",
    "            model_LeNet5.add(layers.Conv2D(filters=120, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "            model_LeNet5.add(layers.Flatten())\n",
    "\n",
    "            model_LeNet5.add(layers.Dense(84, activation='relu'))\n",
    "            model_LeNet5.add(layers.Dense(28, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "            model_LeNet5.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "            model_LeNet5.fit(x_train_32x32, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "\n",
    "            loss_train_LeNet5, acc_train_LeNet5 = model_LeNet5.evaluate(x_train_32x32, y_train, verbose=0)\n",
    "            loss_test_LeNet5, acc_test_LeNet5 = model_LeNet5.evaluate(x_test_32x32, y_test, verbose=0)\n",
    "\n",
    "            #print(acc_train_LeNet5, '^', acc_test_LeNet5)\n",
    "\n",
    "            results_df.loc[len(results_df)] = [str(seed), str(Pooling2D)[47:-2], str(model_LeNet5.name),\n",
    "                                               str(model_LeNet5.count_params()),\n",
    "                                               acc_train_LeNet5, loss_train_LeNet5,\n",
    "                                               acc_test_LeNet5, loss_test_LeNet5]\n",
    "            print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "            y_pred_classes = np.argmax(model_LeNet5.predict(x_test_32x32), axis = 1) \n",
    "            y_classes = np.argmax(y_test, axis = 1)\n",
    "            conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_LeNet5.name + '-' + str(Pooling2D)[47:-2]\n",
    "                     , seed)\n",
    "\n",
    "\n",
    "        # pixels 1\n",
    "        keras.backend.clear_session()    \n",
    "        model_pixels_50_50 = Sequential(name = 'Pixels-400-50-50-28')\n",
    "\n",
    "        model_pixels_50_50.add(layers.Dense(50, input_shape = (400,), activation='relu'))\n",
    "        model_pixels_50_50.add(layers.Dense(50, activation='relu'))\n",
    "        model_pixels_50_50.add(layers.Dense(28, activation='softmax'))\n",
    "\n",
    "        model_pixels_50_50.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "        model_pixels_50_50.fit(x_train_20x20, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "        loss_train_pixels_50_50, acc_train_pixels_50_50 = model_pixels_50_50.evaluate(x_train_20x20, y_train, verbose=0)\n",
    "        loss_test_pixels_50_50, acc_test_pixels_50_50 = model_pixels_50_50.evaluate(x_test_20x20, y_test, verbose=0)\n",
    "\n",
    "        results_df.loc[len(results_df)] = [str(seed), str('-'), str(model_pixels_50_50.name),\n",
    "                                           str(model_pixels_50_50.count_params()),\n",
    "                                           acc_train_pixels_50_50, loss_train_pixels_50_50,\n",
    "                                           acc_test_pixels_50_50, loss_test_pixels_50_50]        \n",
    "        print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "        y_pred_classes = np.argmax(model_pixels_50_50.predict(x_test_20x20), axis = 1) \n",
    "        y_classes = np.argmax(y_test, axis = 1)\n",
    "        conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_pixels_50_50.name, seed)\n",
    "\n",
    "        #pixels 2\n",
    "        keras.backend.clear_session()    \n",
    "        model_pixels_50 = Sequential(name = 'Pixels-400-50-28')\n",
    "\n",
    "        model_pixels_50.add(layers.Dense(50, input_shape = (400,), activation='relu'))\n",
    "        model_pixels_50.add(layers.Dense(28, activation='softmax'))\n",
    "\n",
    "        model_pixels_50.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "        model_pixels_50.fit(x_train_20x20, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "        loss_train_pixels_50_50, acc_train_pixels_50_50 = model_pixels_50.evaluate(x_train_20x20, y_train, verbose=0)\n",
    "        loss_test_pixels_50_50, acc_test_pixels_50_50 = model_pixels_50.evaluate(x_test_20x20, y_test, verbose=0)\n",
    "\n",
    "        results_df.loc[len(results_df)] = [str(seed), str('-'), str(model_pixels_50.name),\n",
    "                                           str(model_pixels_50.count_params()),\n",
    "                                           acc_train_pixels_50_50, loss_train_pixels_50_50,\n",
    "                                           acc_test_pixels_50_50, loss_test_pixels_50_50]        \n",
    "        print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "        y_pred_classes = np.argmax(model_pixels_50.predict(x_test_20x20), axis = 1) \n",
    "        y_classes = np.argmax(y_test, axis = 1)\n",
    "        conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_pixels_50.name, seed)\n",
    "\n",
    "        #pdc 1\n",
    "        keras.backend.clear_session()\n",
    "        model_pdc_50_50 = Sequential(name = 'PDC-256-50-50-28')\n",
    "\n",
    "        model_pdc_50_50.add(layers.Dense(50, input_shape = (256,), activation='relu'))\n",
    "        model_pdc_50_50.add(layers.Dense(50, activation='relu'))\n",
    "        model_pdc_50_50.add(layers.Dense(28, activation='softmax'))\n",
    "\n",
    "        model_pdc_50_50.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "        model_pdc_50_50.fit(x_train_pdc, y_train_pdc, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "        loss_train_pdc_50_50, acc_train_pdc_50_50 = model_pdc_50_50.evaluate(x_train_pdc, y_train_pdc, verbose=0)\n",
    "        loss_test_pdc_50_50, acc_test_pdc_50_50 = model_pdc_50_50.evaluate(x_test_pdc, y_test_pdc, verbose=0)\n",
    "\n",
    "        results_df.loc[len(results_df)] = [str(seed), str('-'), str(model_pdc_50_50.name),\n",
    "                                           str(model_pdc_50_50.count_params()),\n",
    "                                           acc_train_pdc_50_50, loss_train_pdc_50_50,\n",
    "                                           acc_test_pdc_50_50, loss_test_pdc_50_50]        \n",
    "        print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "        y_pred_classes = np.argmax(model_pdc_50_50.predict(x_test_pdc), axis = 1) \n",
    "        y_classes = np.argmax(y_test_pdc, axis = 1)\n",
    "        conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_pdc_50_50.name, seed)\n",
    "\n",
    "        #pdc 2\n",
    "        keras.backend.clear_session()\n",
    "        model_pdc_50 = Sequential(name = 'PDC-256-50-28')\n",
    "\n",
    "        model_pdc_50.add(layers.Dense(50, input_shape = (256,), activation='relu'))\n",
    "        model_pdc_50.add(layers.Dense(28, activation='softmax'))\n",
    "\n",
    "        model_pdc_50.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "        model_pdc_50.fit(x_train_pdc, y_train_pdc, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "        loss_train_pdc_50, acc_train_pdc_50 = model_pdc_50.evaluate(x_train_pdc, y_train_pdc, verbose=0)\n",
    "        loss_test_pdc_50, acc_test_pdc_50 = model_pdc_50.evaluate(x_test_pdc, y_test_pdc, verbose=0)\n",
    "\n",
    "        results_df.loc[len(results_df)] = [str(seed), str('-'), str(model_pdc_50.name),\n",
    "                                           str(model_pdc_50.count_params()),\n",
    "                                           acc_train_pdc_50, loss_train_pdc_50,\n",
    "                                           acc_test_pdc_50, loss_test_pdc_50]        \n",
    "        print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "        y_pred_classes = np.argmax(model_pdc_50.predict(x_test_pdc), axis = 1) \n",
    "        y_classes = np.argmax(y_test_pdc, axis = 1)\n",
    "        conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_pdc_50.name, seed)\n",
    "\n",
    "        results_df.to_csv(r'Results_train-' + str(aug_size * 14) +'k_all-conv-pixels-pdc-added.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUGMENTATION STARTING...\n",
      "DATA READ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rustem\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\Rustem\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras_preprocessing\\image\\image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUGMENTATION DONE 42000\n",
      "PDC READ\n",
      "SIZES DONE!\n",
      "CLEARSESSION STARTED\n",
      "CLEARSESSION DONE\n",
      "MODEL STR DEFINED\n",
      "FITTED\n",
      "['0', 'AveragePooling2D', 'LeNet5', '63236', 0.9913333058357239, 0.02424595132470131, 0.871071457862854, 1.4405299425125122]\n",
      "['0', 'AveragePooling2D', 'Conv-14x14', '61080', 0.9869285821914673, 0.03507982939481735, 0.783214271068573, 1.9217894077301025]\n",
      "['0', 'AveragePooling2D', 'LeNet5-filters_6-8', '38028', 0.9913809299468994, 0.02406124956905842, 0.8739285469055176, 1.2112934589385986]\n",
      "['0', 'AveragePooling2D', 'LeNet5-filters_6-4', '25424', 0.991190493106842, 0.024006469175219536, 0.866428554058075, 1.2757506370544434]\n",
      "CLEARSESSION STARTED\n",
      "CLEARSESSION DONE\n",
      "MODEL STR DEFINED\n",
      "FITTED\n",
      "['0', 'MaxPooling2D', 'LeNet5', '63236', 0.9933333396911621, 0.02050248719751835, 0.8739285469055176, 1.3196334838867188]\n",
      "['0', 'MaxPooling2D', 'Conv-14x14', '61080', 0.9902142882347107, 0.027915742248296738, 0.7814285755157471, 2.15394926071167]\n",
      "['0', 'MaxPooling2D', 'LeNet5-filters_6-8', '38028', 0.9921904802322388, 0.02311895601451397, 0.875, 1.3758656978607178]\n",
      "['0', 'MaxPooling2D', 'LeNet5-filters_6-4', '25424', 0.9881190657615662, 0.032755304127931595, 0.8703571557998657, 1.5301440954208374]\n",
      "['0', '-', 'Pixels-400-50-50-28', '24028', 0.9904524087905884, 0.028861133381724358, 0.8267857432365417, 1.5785449743270874]\n",
      "['0', '-', 'Pixels-400-50-28', '21478', 0.9898809790611267, 0.036980126053094864, 0.8278571367263794, 1.5153604745864868]\n",
      "['0', '-', 'PDC-256-50-50-28', '16828', 0.9858333468437195, 0.0382566973567009, 0.8753571510314941, 1.4965461492538452]\n",
      "['0', '-', 'PDC-256-50-28', '14278', 0.9848095178604126, 0.041224170476198196, 0.868571400642395, 1.3200618028640747]\n",
      "CLEARSESSION STARTED\n",
      "CLEARSESSION DONE\n",
      "MODEL STR DEFINED\n",
      "FITTED\n",
      "['1', 'AveragePooling2D', 'LeNet5', '63236', 0.9919285774230957, 0.023935390636324883, 0.8714285492897034, 1.4660276174545288]\n",
      "['1', 'AveragePooling2D', 'Conv-14x14', '61080', 0.9931904673576355, 0.020550217479467392, 0.7932142615318298, 2.2542848587036133]\n",
      "['1', 'AveragePooling2D', 'LeNet5-filters_6-8', '38028', 0.9890238046646118, 0.032764896750450134, 0.8742856979370117, 1.1331626176834106]\n",
      "['1', 'AveragePooling2D', 'LeNet5-filters_6-4', '25424', 0.9922142624855042, 0.023178881034255028, 0.8796428442001343, 1.180686593055725]\n",
      "CLEARSESSION STARTED\n",
      "CLEARSESSION DONE\n",
      "MODEL STR DEFINED\n",
      "FITTED\n",
      "['1', 'MaxPooling2D', 'LeNet5', '63236', 0.9945952296257019, 0.016021793708205223, 0.8842856884002686, 1.1113529205322266]\n",
      "['1', 'MaxPooling2D', 'Conv-14x14', '61080', 0.9931666851043701, 0.02105364389717579, 0.8007143139839172, 1.9013644456863403]\n",
      "['1', 'MaxPooling2D', 'LeNet5-filters_6-8', '38028', 0.9941904544830322, 0.014390287920832634, 0.8778571486473083, 1.1719014644622803]\n",
      "['1', 'MaxPooling2D', 'LeNet5-filters_6-4', '25424', 0.9913095235824585, 0.024245034903287888, 0.8725000023841858, 1.3740055561065674]\n",
      "['1', '-', 'Pixels-400-50-50-28', '24028', 0.987928569316864, 0.03400568664073944, 0.8307142853736877, 1.8992691040039062]\n",
      "['1', '-', 'Pixels-400-50-28', '21478', 0.9911428689956665, 0.03638080880045891, 0.8196428418159485, 1.5287593603134155]\n",
      "['1', '-', 'PDC-256-50-50-28', '16828', 0.9854999780654907, 0.04095873609185219, 0.8692857027053833, 1.416306734085083]\n",
      "['1', '-', 'PDC-256-50-28', '14278', 0.9861666560173035, 0.039332278072834015, 0.8621428608894348, 1.5255330801010132]\n",
      "CLEARSESSION STARTED\n",
      "CLEARSESSION DONE\n",
      "MODEL STR DEFINED\n",
      "FITTED\n",
      "['2', 'AveragePooling2D', 'LeNet5', '63236', 0.9952619075775146, 0.014485128223896027, 0.8803571462631226, 1.0335139036178589]\n",
      "['2', 'AveragePooling2D', 'Conv-14x14', '61080', 0.9923571348190308, 0.020827746018767357, 0.789642870426178, 1.9942262172698975]\n",
      "['2', 'AveragePooling2D', 'LeNet5-filters_6-8', '38028', 0.9936666488647461, 0.019418375566601753, 0.8721428513526917, 1.3041794300079346]\n",
      "['2', 'AveragePooling2D', 'LeNet5-filters_6-4', '25424', 0.9938571453094482, 0.018491849303245544, 0.8767856955528259, 1.210221529006958]\n",
      "CLEARSESSION STARTED\n",
      "CLEARSESSION DONE\n",
      "MODEL STR DEFINED\n",
      "FITTED\n",
      "['2', 'MaxPooling2D', 'LeNet5', '63236', 0.9934999942779541, 0.018984263762831688, 0.8760714530944824, 1.2858773469924927]\n",
      "['2', 'MaxPooling2D', 'Conv-14x14', '61080', 0.9961428642272949, 0.013221829198300838, 0.791785717010498, 2.1963741779327393]\n",
      "['2', 'MaxPooling2D', 'LeNet5-filters_6-8', '38028', 0.9956904649734497, 0.012510999105870724, 0.8728571534156799, 1.348819375038147]\n",
      "['2', 'MaxPooling2D', 'LeNet5-filters_6-4', '25424', 0.9916666746139526, 0.023313449695706367, 0.8774999976158142, 1.2328976392745972]\n",
      "['2', '-', 'Pixels-400-50-50-28', '24028', 0.9927380681037903, 0.02226073481142521, 0.831428587436676, 1.9680683612823486]\n",
      "['2', '-', 'Pixels-400-50-28', '21478', 0.9919523596763611, 0.03425639122724533, 0.8303571343421936, 1.4100768566131592]\n",
      "['2', '-', 'PDC-256-50-50-28', '16828', 0.9748095273971558, 0.07019953429698944, 0.8639285564422607, 1.491224765777588]\n",
      "['2', '-', 'PDC-256-50-28', '14278', 0.9869285821914673, 0.03595355153083801, 0.8732143044471741, 1.6366618871688843]\n",
      "CLEARSESSION STARTED\n",
      "CLEARSESSION DONE\n",
      "MODEL STR DEFINED\n",
      "FITTED\n",
      "['3', 'AveragePooling2D', 'LeNet5', '63236', 0.9921666383743286, 0.021121034398674965, 0.8782142996788025, 1.326509714126587]\n",
      "['3', 'AveragePooling2D', 'Conv-14x14', '61080', 0.9937857389450073, 0.02095312625169754, 0.7892857193946838, 2.026134967803955]\n",
      "['3', 'AveragePooling2D', 'LeNet5-filters_6-8', '38028', 0.9953095316886902, 0.013426611199975014, 0.8796428442001343, 1.241072177886963]\n",
      "['3', 'AveragePooling2D', 'LeNet5-filters_6-4', '25424', 0.9900952577590942, 0.028222957625985146, 0.8675000071525574, 1.3004026412963867]\n",
      "CLEARSESSION STARTED\n",
      "CLEARSESSION DONE\n",
      "MODEL STR DEFINED\n",
      "FITTED\n",
      "['3', 'MaxPooling2D', 'LeNet5', '63236', 0.9923095107078552, 0.020574219524860382, 0.8882142901420593, 1.0123608112335205]\n",
      "['3', 'MaxPooling2D', 'Conv-14x14', '61080', 0.9929285645484924, 0.01989029161632061, 0.7464285492897034, 2.970073699951172]\n",
      "['3', 'MaxPooling2D', 'LeNet5-filters_6-8', '38028', 0.989642858505249, 0.03120492957532406, 0.8728571534156799, 1.3740514516830444]\n",
      "['3', 'MaxPooling2D', 'LeNet5-filters_6-4', '25424', 0.9903095364570618, 0.02780214510858059, 0.8632143139839172, 1.2098972797393799]\n",
      "['3', '-', 'Pixels-400-50-50-28', '24028', 0.9920952320098877, 0.024579115211963654, 0.8324999809265137, 1.7108323574066162]\n",
      "['3', '-', 'Pixels-400-50-28', '21478', 0.9917142987251282, 0.03395703807473183, 0.8199999928474426, 1.577417016029358]\n",
      "['3', '-', 'PDC-256-50-50-28', '16828', 0.9878571629524231, 0.03472767770290375, 0.8650000095367432, 1.6171578168869019]\n",
      "['3', '-', 'PDC-256-50-28', '14278', 0.9866904616355896, 0.037887681275606155, 0.8714285492897034, 1.4147194623947144]\n",
      "CLEARSESSION STARTED\n",
      "CLEARSESSION DONE\n",
      "MODEL STR DEFINED\n",
      "FITTED\n",
      "['4', 'AveragePooling2D', 'LeNet5', '63236', 0.9951428771018982, 0.013947985135018826, 0.885357141494751, 1.0790389776229858]\n",
      "['4', 'AveragePooling2D', 'Conv-14x14', '61080', 0.9923571348190308, 0.022964127361774445, 0.8057143092155457, 1.8480993509292603]\n",
      "['4', 'AveragePooling2D', 'LeNet5-filters_6-8', '38028', 0.9900476336479187, 0.029172522947192192, 0.8764285445213318, 1.0442341566085815]\n",
      "['4', 'AveragePooling2D', 'LeNet5-filters_6-4', '25424', 0.9884762167930603, 0.03093254379928112, 0.8603571653366089, 1.3190926313400269]\n",
      "CLEARSESSION STARTED\n",
      "CLEARSESSION DONE\n",
      "MODEL STR DEFINED\n",
      "FITTED\n",
      "['4', 'MaxPooling2D', 'LeNet5', '63236', 0.9968095421791077, 0.008733750320971012, 0.8846428394317627, 1.1399784088134766]\n",
      "['4', 'MaxPooling2D', 'Conv-14x14', '61080', 0.9929999709129333, 0.02067025750875473, 0.8035714030265808, 1.9834802150726318]\n",
      "['4', 'MaxPooling2D', 'LeNet5-filters_6-8', '38028', 0.9978571534156799, 0.006281611975282431, 0.875, 1.6924546957015991]\n",
      "['4', 'MaxPooling2D', 'LeNet5-filters_6-4', '25424', 0.9927856922149658, 0.02097056433558464, 0.8728571534156799, 1.4399490356445312]\n",
      "['4', '-', 'Pixels-400-50-50-28', '24028', 0.9890952110290527, 0.031679604202508926, 0.8335714340209961, 1.5796948671340942]\n",
      "['4', '-', 'Pixels-400-50-28', '21478', 0.9896904826164246, 0.037444282323122025, 0.8264285922050476, 1.536368727684021]\n",
      "['4', '-', 'PDC-256-50-50-28', '16828', 0.9869047403335571, 0.03727186098694801, 0.8689285516738892, 1.600890040397644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '-', 'PDC-256-50-28', '14278', 0.9876904487609863, 0.03584042936563492, 0.8732143044471741, 1.2610753774642944]\n"
     ]
    }
   ],
   "source": [
    "def conf_2x2(Y_classes, Y_pred_classes, label_map, model_name, seed):\n",
    "    return\n",
    "    \n",
    "# Tezis 1.2\n",
    "\n",
    "EPOCHS = 200\n",
    "callback = EarlyStopping(monitor='loss', min_delta=0.01, patience=5)\n",
    "\n",
    "#models = []\n",
    "aug_size = 3 \n",
    "results_df = pd.DataFrame(columns=['Seed', 'Pooling2D', 'Model', 'Param-Count',\n",
    "                               'Accuracy-Train', 'Loss-Train', 'Accuracy-Test', 'Loss-Test'])\n",
    "print('AUGMENTATION STARTING...')\n",
    "x_train_original, y_train_original = read_preapare_db_original_size(Path('datasetsforfinalresults/HandChars_500_woI_Train.txt').absolute())\n",
    "print('DATA READ!')\n",
    "x_train_original, y_train_original = augment(x_train_original, y_train_original, n=aug_size, is_shuffled = False) # n * 14000\n",
    "print('AUGMENTATION DONE', str(x_train_original.shape[0]))\n",
    "x_train_pdc, y_train_pdc = read_prepare_db_pdc(Path(aug_switch.get(aug_size, '')).absolute(), (0, 256, 1), (0, 256, 1))\n",
    "print('PDC READ')\n",
    "y_train = y_train_original\n",
    "x_train_32x32 = extract_spec_size(x_train_original, (20, 20, 1), (32, 32, 1))\n",
    "x_train_14x14 = extract_spec_size(x_train_original, (10, 10, 1), (14, 14, 1))\n",
    "x_train_20x20 = extract_spec_size(x_train_original, (20, 20, 1), (20, 20, 1))\n",
    "\n",
    "#x_train_new = extract_spec_size(x_train_original, (32, 32, 1), (32, 32, 1)) #(20, 20, 1), (32, 32, 1)\n",
    "#x_train_t = x_train_new\n",
    "#y_train_t = y_train_new\n",
    "#plt.imshow(x_train_new[14000 + 455*28].reshape(32, 32),cmap='Greys')\n",
    "#write_to_file(Path('datasets/500-100/GENERATED_HandChars_' + str(n_i*14) + 'k_32x32_Train-18-iyun.txt').absolute(), x_train_new, y_train_new)\n",
    "\n",
    "\n",
    "\n",
    "x_train_20x20 = x_train_20x20.reshape(x_train_20x20.shape[0], 400)\n",
    "print('SIZES DONE!')\n",
    "for seed in range(5):\n",
    "    for Pooling2D in (layers.AveragePooling2D,layers.MaxPooling2D):   #layers.MaxPooling2D, \n",
    "        set_seed(seed)\n",
    "        print('CLEARSESSION STARTED')\n",
    "        # conv 1\n",
    "        keras.backend.clear_session()\n",
    "        print('CLEARSESSION DONE')\n",
    "        model_LeNet5 = Sequential(name = 'LeNet5')\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=6,  kernel_size=(5,5), padding='valid', activation='relu', input_shape=(32, 32, 1)))\n",
    "        model_LeNet5.add(Pooling2D(pool_size = (2,2)))\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "        model_LeNet5.add(Pooling2D(pool_size=(2,2)))\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=120, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "        model_LeNet5.add(layers.Flatten())\n",
    "\n",
    "        model_LeNet5.add(layers.Dense(84, activation='relu'))\n",
    "        model_LeNet5.add(layers.Dense(28, activation=\"softmax\"))\n",
    "\n",
    "        print('MODEL STR DEFINED')\n",
    "        model_LeNet5.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "        model_LeNet5.fit(x_train_32x32, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "        #kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "        # len(fitted_results.history['loss'])\n",
    "        print('FITTED')\n",
    "        loss_train_LeNet5, acc_train_LeNet5 = model_LeNet5.evaluate(x_train_32x32, y_train, verbose=0)\n",
    "        loss_test_LeNet5, acc_test_LeNet5 = model_LeNet5.evaluate(x_test_32x32, y_test, verbose=0)\n",
    "        \n",
    "        #models.append(model_LeNet5)\n",
    "        #print(acc_train_LeNet5, '^', acc_test_LeNet5)\n",
    "\n",
    "        results_df.loc[len(results_df)] = [str(seed), str(Pooling2D)[47:-2], str(model_LeNet5.name),\n",
    "                                           str(model_LeNet5.count_params()),\n",
    "                                           acc_train_LeNet5, loss_train_LeNet5,\n",
    "                                           acc_test_LeNet5, loss_test_LeNet5]\n",
    "        print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "        y_pred_classes = np.argmax(model_LeNet5.predict(x_test_32x32), axis = 1) \n",
    "        y_classes = np.argmax(y_test, axis = 1)\n",
    "        conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_LeNet5.name + '-' + str(Pooling2D)[47:-2]\n",
    "                 , seed)\n",
    "\n",
    "        # conv 2\n",
    "        keras.backend.clear_session()\n",
    "        model_LeNet5 = Sequential(name = 'Conv-14x14')\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='relu', input_shape=(14, 14, 1)))\n",
    "        model_LeNet5.add(Pooling2D(pool_size=(2,2)))\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=120, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "        model_LeNet5.add(layers.Flatten())\n",
    "\n",
    "        model_LeNet5.add(layers.Dense(84, activation='relu'))\n",
    "        model_LeNet5.add(layers.Dense(28, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "        model_LeNet5.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "        model_LeNet5.fit(x_train_14x14, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "\n",
    "        loss_train_LeNet5, acc_train_LeNet5 = model_LeNet5.evaluate(x_train_14x14, y_train, verbose=0)\n",
    "        loss_test_LeNet5, acc_test_LeNet5 = model_LeNet5.evaluate(x_test_14x14, y_test, verbose=0)\n",
    "\n",
    "        results_df.loc[len(results_df)] = [str(seed), str(Pooling2D)[47:-2], str(model_LeNet5.name),\n",
    "                                           str(model_LeNet5.count_params()),\n",
    "                                           acc_train_LeNet5, loss_train_LeNet5,\n",
    "                                           acc_test_LeNet5, loss_test_LeNet5]\n",
    "        print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "        y_pred_classes = np.argmax(model_LeNet5.predict(x_test_14x14), axis = 1) \n",
    "        y_classes = np.argmax(y_test, axis = 1)\n",
    "        conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_LeNet5.name + '-' + str(Pooling2D)[47:-2]\n",
    "                 , seed)\n",
    "\n",
    "        # conv 3\n",
    "        keras.backend.clear_session()\n",
    "        model_LeNet5 = Sequential(name = 'LeNet5-filters_6-8')\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=6,  kernel_size=(5,5), padding='valid', activation='relu', input_shape=(32, 32, 1)))\n",
    "        model_LeNet5.add(Pooling2D(pool_size = (2,2)))\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=8, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "        model_LeNet5.add(Pooling2D(pool_size=(2,2)))\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=120, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "        model_LeNet5.add(layers.Flatten())\n",
    "\n",
    "        model_LeNet5.add(layers.Dense(84, activation='relu'))\n",
    "        model_LeNet5.add(layers.Dense(28, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "        model_LeNet5.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "        model_LeNet5.fit(x_train_32x32, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "\n",
    "        loss_train_LeNet5, acc_train_LeNet5 = model_LeNet5.evaluate(x_train_32x32, y_train, verbose=0)\n",
    "        loss_test_LeNet5, acc_test_LeNet5 = model_LeNet5.evaluate(x_test_32x32, y_test, verbose=0)\n",
    "\n",
    "        #print(acc_train_LeNet5, '^', acc_test_LeNet5)\n",
    "\n",
    "        results_df.loc[len(results_df)] = [str(seed), str(Pooling2D)[47:-2], str(model_LeNet5.name),\n",
    "                                           str(model_LeNet5.count_params()),\n",
    "                                           acc_train_LeNet5, loss_train_LeNet5,\n",
    "                                           acc_test_LeNet5, loss_test_LeNet5]\n",
    "        print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "        y_pred_classes = np.argmax(model_LeNet5.predict(x_test_32x32), axis = 1) \n",
    "        y_classes = np.argmax(y_test, axis = 1)\n",
    "        conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_LeNet5.name + '-' + str(Pooling2D)[47:-2]\n",
    "                 , seed)\n",
    "\n",
    "        # conv 4\n",
    "        keras.backend.clear_session()\n",
    "        model_LeNet5 = Sequential(name = 'LeNet5-filters_6-4')\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=6,  kernel_size=(5,5), padding='valid', activation='relu', input_shape=(32, 32, 1)))\n",
    "        model_LeNet5.add(Pooling2D(pool_size = (2,2)))\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=4, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "        model_LeNet5.add(Pooling2D(pool_size=(2,2)))\n",
    "\n",
    "        model_LeNet5.add(layers.Conv2D(filters=120, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "        model_LeNet5.add(layers.Flatten())\n",
    "\n",
    "        model_LeNet5.add(layers.Dense(84, activation='relu'))\n",
    "        model_LeNet5.add(layers.Dense(28, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "        model_LeNet5.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "        model_LeNet5.fit(x_train_32x32, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "\n",
    "        loss_train_LeNet5, acc_train_LeNet5 = model_LeNet5.evaluate(x_train_32x32, y_train, verbose=0)\n",
    "        loss_test_LeNet5, acc_test_LeNet5 = model_LeNet5.evaluate(x_test_32x32, y_test, verbose=0)\n",
    "\n",
    "        #print(acc_train_LeNet5, '^', acc_test_LeNet5)\n",
    "\n",
    "        results_df.loc[len(results_df)] = [str(seed), str(Pooling2D)[47:-2], str(model_LeNet5.name),\n",
    "                                           str(model_LeNet5.count_params()),\n",
    "                                           acc_train_LeNet5, loss_train_LeNet5,\n",
    "                                           acc_test_LeNet5, loss_test_LeNet5]\n",
    "        print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "        y_pred_classes = np.argmax(model_LeNet5.predict(x_test_32x32), axis = 1) \n",
    "        y_classes = np.argmax(y_test, axis = 1)\n",
    "        conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_LeNet5.name + '-' + str(Pooling2D)[47:-2]\n",
    "                 , seed)\n",
    "\n",
    "\n",
    "    # pixels 1\n",
    "    keras.backend.clear_session()    \n",
    "    model_pixels_50_50 = Sequential(name = 'Pixels-400-50-50-28')\n",
    "\n",
    "    model_pixels_50_50.add(layers.Dense(50, input_shape = (400,), activation='relu'))\n",
    "    model_pixels_50_50.add(layers.Dense(50, activation='relu'))\n",
    "    model_pixels_50_50.add(layers.Dense(28, activation='softmax'))\n",
    "\n",
    "    model_pixels_50_50.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "    model_pixels_50_50.fit(x_train_20x20, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "    loss_train_pixels_50_50, acc_train_pixels_50_50 = model_pixels_50_50.evaluate(x_train_20x20, y_train, verbose=0)\n",
    "    loss_test_pixels_50_50, acc_test_pixels_50_50 = model_pixels_50_50.evaluate(x_test_20x20, y_test, verbose=0)\n",
    "\n",
    "    results_df.loc[len(results_df)] = [str(seed), str('-'), str(model_pixels_50_50.name),\n",
    "                                       str(model_pixels_50_50.count_params()),\n",
    "                                       acc_train_pixels_50_50, loss_train_pixels_50_50,\n",
    "                                       acc_test_pixels_50_50, loss_test_pixels_50_50]        \n",
    "    print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "    y_pred_classes = np.argmax(model_pixels_50_50.predict(x_test_20x20), axis = 1) \n",
    "    y_classes = np.argmax(y_test, axis = 1)\n",
    "    conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_pixels_50_50.name, seed)\n",
    "\n",
    "    #pixels 2\n",
    "    keras.backend.clear_session()    \n",
    "    model_pixels_50 = Sequential(name = 'Pixels-400-50-28')\n",
    "\n",
    "    model_pixels_50.add(layers.Dense(50, input_shape = (400,), activation='relu'))\n",
    "    model_pixels_50.add(layers.Dense(28, activation='softmax'))\n",
    "\n",
    "    model_pixels_50.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "    model_pixels_50.fit(x_train_20x20, y_train, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "    loss_train_pixels_50_50, acc_train_pixels_50_50 = model_pixels_50.evaluate(x_train_20x20, y_train, verbose=0)\n",
    "    loss_test_pixels_50_50, acc_test_pixels_50_50 = model_pixels_50.evaluate(x_test_20x20, y_test, verbose=0)\n",
    "\n",
    "    results_df.loc[len(results_df)] = [str(seed), str('-'), str(model_pixels_50.name),\n",
    "                                       str(model_pixels_50.count_params()),\n",
    "                                       acc_train_pixels_50_50, loss_train_pixels_50_50,\n",
    "                                       acc_test_pixels_50_50, loss_test_pixels_50_50]        \n",
    "    print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "    y_pred_classes = np.argmax(model_pixels_50.predict(x_test_20x20), axis = 1) \n",
    "    y_classes = np.argmax(y_test, axis = 1)\n",
    "    conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_pixels_50.name, seed)\n",
    "\n",
    "    #pdc 1\n",
    "    keras.backend.clear_session()\n",
    "    model_pdc_50_50 = Sequential(name = 'PDC-256-50-50-28')\n",
    "\n",
    "    model_pdc_50_50.add(layers.Dense(50, input_shape = (256,), activation='relu'))\n",
    "    model_pdc_50_50.add(layers.Dense(50, activation='relu'))\n",
    "    model_pdc_50_50.add(layers.Dense(28, activation='softmax'))\n",
    "\n",
    "    model_pdc_50_50.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "    model_pdc_50_50.fit(x_train_pdc, y_train_pdc, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "    loss_train_pdc_50_50, acc_train_pdc_50_50 = model_pdc_50_50.evaluate(x_train_pdc, y_train_pdc, verbose=0)\n",
    "    loss_test_pdc_50_50, acc_test_pdc_50_50 = model_pdc_50_50.evaluate(x_test_pdc, y_test_pdc, verbose=0)\n",
    "\n",
    "    results_df.loc[len(results_df)] = [str(seed), str('-'), str(model_pdc_50_50.name),\n",
    "                                       str(model_pdc_50_50.count_params()),\n",
    "                                       acc_train_pdc_50_50, loss_train_pdc_50_50,\n",
    "                                       acc_test_pdc_50_50, loss_test_pdc_50_50]        \n",
    "    print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "    y_pred_classes = np.argmax(model_pdc_50_50.predict(x_test_pdc), axis = 1) \n",
    "    y_classes = np.argmax(y_test_pdc, axis = 1)\n",
    "    conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_pdc_50_50.name, seed)\n",
    "\n",
    "    #pdc 2\n",
    "    keras.backend.clear_session()\n",
    "    model_pdc_50 = Sequential(name = 'PDC-256-50-28')\n",
    "\n",
    "    model_pdc_50.add(layers.Dense(50, input_shape = (256,), activation='relu'))\n",
    "    model_pdc_50.add(layers.Dense(28, activation='softmax'))\n",
    "\n",
    "    model_pdc_50.compile(optimizer = 'Adam' , loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "    model_pdc_50.fit(x_train_pdc, y_train_pdc, epochs = EPOCHS, shuffle = True, callbacks=[callback], verbose=0) #callbacks=[callback],\n",
    "\n",
    "    loss_train_pdc_50, acc_train_pdc_50 = model_pdc_50.evaluate(x_train_pdc, y_train_pdc, verbose=0)\n",
    "    loss_test_pdc_50, acc_test_pdc_50 = model_pdc_50.evaluate(x_test_pdc, y_test_pdc, verbose=0)\n",
    "\n",
    "    results_df.loc[len(results_df)] = [str(seed), str('-'), str(model_pdc_50.name),\n",
    "                                       str(model_pdc_50.count_params()),\n",
    "                                       acc_train_pdc_50, loss_train_pdc_50,\n",
    "                                       acc_test_pdc_50, loss_test_pdc_50]        \n",
    "    print(results_df.loc[len(results_df)-1,:].tolist())\n",
    "\n",
    "    y_pred_classes = np.argmax(model_pdc_50.predict(x_test_pdc), axis = 1) \n",
    "    y_classes = np.argmax(y_test_pdc, axis = 1)\n",
    "    conf_2x2(y_classes, y_pred_classes, 'ABCÇDEƏFGHIJKQLMNOPRSŞTUVXYZ', model_pdc_50.name, seed)\n",
    "\n",
    "    results_df.to_csv(r'Results_train-' + str(aug_size * 14) +'k_all-conv-pixels-pdc-added.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
